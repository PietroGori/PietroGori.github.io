<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Self-supervised Learning | Pietro Gori </title> <meta name="author" content="Pietro Gori"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logoTP.png?8ebccf6bd87915c074042a2dc58c272b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pietrogori.github.io/projects/SSL"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Pietro</span> Gori </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">Team </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Self-supervised Learning</h1> <p class="post-description"></p> </header> <article> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Team-CL-480.webp 480w,/assets/img/Team-CL-800.webp 800w,/assets/img/Team-CL-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Team-CL.png" width="80%" height="auto" style=" max-width: 100%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Context</strong> Many specific tasks in Computer Vision, such as object detection (e.g., YOLOv71), image classification (e.g., ResNet-50), or semantic segmentation (e.g., UNet, Swin Transformer) have reached astonishing results in the last years. This has been possible mainly because large (N &gt; \(10^6\)), labeled data-sets were easily accessible and freely available. However, many applications in medical imaging lack such large datasets and annotation might be very time-consuming and difficult even for experienced medical doctors. For instance, predicting mental or neurodevelopmental disorders from neuroanatomical imaging data (e.g., T1-w MRI) has not yet achieved the expected results (i.e., AUC ≥ 90 <a class="citation" href="#dufumier_exploring_2024">(Dufumier et al., 2024)</a>). Furthermore, recent studies yielded contradictory results when comparing Deep Learning with Standard Machine Learning (SML) on top of classical feature extraction <a class="citation" href="#dufumier_exploring_2024">(Dufumier et al., 2024)</a>.</p> <p><strong>Challenges</strong> The first challenge concerns the small number of pathological samples. In supervised learning, when dealing with a small labeled dataset, the most used and well-known solution is supervised Transfer Learning from ImageNet (or other large vision datasets). However, it has been recently shown that this strategy is useful, namely features are re-used, only when there is a high visual similarity between the pre-train and target domain (e.g., low Fréchet inception distance (FID)). This is not the case when comparing natural and medical images. Furthermore, many medical images, and in particular brain MRI scans, are 3D volumes, differently from the 2D images of ImageNet. This entails a great domain gap between the large labeled datasets used in computer vision and medical images. Another approach comprises self-supervised learning (SSL) methods which leverage an annotation-free pretext task to provide a surrogate supervision signal for feature learning. Nonetheless, these methods still need large (unannotated) datasets, which should comprise, to reduce the domain gap, data similar to the ones in the (labeled) target dataset, namely pathological patients. However, the large majority of images currently stored in hospitals and clinical laboratories belong to healthy subjects. Indeed, the largest datasets currently available (e.g., UKBioBank and OpenBHB) mostly contain data of healthy subjects. Furthermore, these datasets usually comprise one or multiple imaging modalities, as well as clinical data, such as age, gender and weight. The research challenge thus becomes how to leverage large datasets of healthy subjects and combine the heterogeneous sources of information (i.e., clinical and imaging data) to improve the diagnostic and understanding of patients.</p> <p>A second challenge concerns the data biases. In our work, we define data biases as the visual patterns that correlate with the target task and/or are easy to learn, but are not relevant for the target task. For instance, the site effect in MRI images refers to systematic variations or discrepancies in feature distributions across different imaging sites, that arise from differences in equipment, protocols, or settings , and are not related to a disease (i.e., target task). When working with MRI samples in a binary classification problem (healthy Vs patients), these spurious differences can be visually more accentuated, and thus easy to learn, than the relevant differences between the two classes. This can result in a biased model, whose predictions majorly rely on the bias attributes and not on the true, generalizable, and discriminative features.</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/toy_example_transfer_learning_v2-480.webp 480w,/assets/img/toy_example_transfer_learning_v2-800.webp 800w,/assets/img/toy_example_transfer_learning_v2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/toy_example_transfer_learning_v2.jpg" width="80%" height="auto" style=" max-width: 100%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> In this project, we propose a new paradigm where, instead than learning a network from scratch using a small pathological dataset, we first pre-train it on a large dataset of healthy subjects, thus learning a representation space describing the healthy population. We leverage our new weakly-supervised contrastive loss, combining clinical, biological and imaging data. Then, we transfer it and fine-tune it on the small pathological dataset. </div> <p><strong>Contributions</strong> In this project, we have proposed a new geometric approach for contrastive learning <a class="citation" href="#barbano_unbiased_2023">(Barbano et al., 2023)</a> that can be used in different settings:</p> <ol> <li>unsupervised (i.e., no labels) <a class="citation" href="#sarfati_learning_2023">(Sarfati et al., 2023)</a>, <a class="citation" href="#ruppli_optimizing_2022">(Ruppli et al., 2022)</a>, <a class="citation" href="#barbano_unbiased_2023">(Barbano et al., 2023)</a>,</li> <li>supervised (i.e., class labels) <a class="citation" href="#barbano_unbiased_2023">(Barbano et al., 2023)</a>, and</li> <li>weakly-supervised (i.e., weak attributes or regression) <a class="citation" href="#dufumier_contrastive_2021">(Dufumier et al., 2021)</a> <a class="citation" href="#barbano_contrastive_2023">(Barbano et al., 2023)</a> <a class="citation" href="#dufumier_integrating_2023">(Dufumier et al., 2023)</a>, <a class="citation" href="#ruppli_decoupled_2023">(Ruppli et al., 2023)</a>, <a class="citation" href="#dufumier_conditional_2021">(Dufumier et al., 2021)</a>.</li> </ol> <p>It is well adapted to integrate prior information, such as weak attributes or representations learned from generative models, and can thus be used to learn a representation of the healthy population by leveraging both clinical and imaging data.</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/geometric-CL-480.webp 480w,/assets/img/geometric-CL-800.webp 800w,/assets/img/geometric-CL-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/geometric-CL.png" width="80%" height="auto" style=" max-width: 100%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> In a) we show a visual explanation of the proposed geometric approach for Contrastive Learning. We aim at increasing the minimal margin ϵ, between the distance d+ of a positive sample x+ (+ symbol inside and yellow color) from an anchor x and the distance d− of the closest negative sample x− (− symbol inside and blue color). By increasing the margin, we can achieve a better separation between positive and negative samples. In b) and c), we show two different scenarios without margin (b) and with margin (c). Filling colors of datapoints represent different biases. In both b) and c) the contrastive conditions are fulfilled and thus the loss is minimized (i.e., positives are closer to the anchor than the negatives). However, we observe that, without imposing a margin, biased clusters might appear containing both positive and negative samples (b). This issue can be mitigated by increasing the ϵ margin (c) and using the proposed regularization loss Fair KL <a class="citation" href="#barbano_unbiased_2023">(Barbano et al., 2023)</a>. </div> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pic-MICCAI-2021-480.webp 480w,/assets/img/pic-MICCAI-2021-800.webp 800w,/assets/img/pic-MICCAI-2021-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/pic-MICCAI-2021.png" width="70%" height="auto" style=" max-width: 100%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Using standard Contrastive Learning losses (e.g., SimCLR), samples are uniformly scattered in the representation space, without considering their clinical variables (metadata, y), such as age. By employing our weakly-supervised losses, two subjects with similar metadata are mapped closer in the representation space compared to two subjects with different metadata. </div> <p>Based on the proposed geometric approach, we show why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data and derive a new debiasing regularization loss, that work well even with extremely biased data. <a class="citation" href="#barbano_unbiased_2023">(Barbano et al., 2023)</a>. You can find a visual explanation below using the color-MNIST dataset.</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bias-480.webp 480w,/assets/img/bias-800.webp 800w,/assets/img/bias-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/bias.png" width="80%" height="auto" style=" max-width: 100%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> A positive bias-aligned sample x+,b is semantically similar (positive) to the anchor (same digit) but it has also the same bias b (yellow color). A positive bias-conflicting sample shares the same digit but it has a different bias b′(different color). Here, the color is defined as a data bias since it’s a visual feature that is correlated with the semantic content related to the target task (digit recognition), but it doesn’t characterize it. </div> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bias-distance-480.webp 480w,/assets/img/bias-distance-800.webp 800w,/assets/img/bias-distance-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/bias-distance.png" width="100%" height="auto" style=" max-width: 100%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> In <a class="citation" href="#barbano_unbiased_2023">(Barbano et al., 2023)</a>, we propose the FairKL regularization term for debiasing. Ideally, we would like that the distances between all positive (resp. negative) samples and the anchor, whatever their bias, are equal. However, this condition is very strict, as it would enforce uniform distance among all positive (resp. negative) samples. We have proposed a more relaxed condition where we force the distributions of distances of positives with different biases to be similar (same for negative). Assuming that the distance distributions follow a normal distribution, we propose to minimize the Kullback-Leibler divergence of the two distributions obtaining a closed form solution that we called FairKL. </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeuroImage</abbr> </div> <div id="dufumier_exploring_2024" class="col-sm-8"> <div class="title">Exploring the potential of representation and transfer learning for anatomical neuroimaging: Application to psychiatry</div> <div class="author"> Benoit Dufumier, <em>Pietro Gori</em>, Sara Petiton, Robin Louiset, Jean-François Mangin, Antoine Grigis, and Edouard Duchesnay </div> <div class="periodical"> <em>NeuroImage</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1016/j.neuroimage.2024.120665" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.sciencedirect.com/science/article/pii/S1053811924001605" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="barbano_unbiased_2023" class="col-sm-8"> <div class="title">Unbiased Supervised Contrastive Learning</div> <div class="author"> Carlo Alberto Barbano, Benoit Dufumier, Enzo Tartaglione, Marco Grangetto, and <em>Pietro Gori</em> </div> <div class="periodical"> <em>In The Eleventh International Conference on Learning Representations (ICLR)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/2211.05568" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/EIDOSLAB/unbiased-contrastive-learning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss ({}epsilon\-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets including CIFAR10, CIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with {}epsilon\-SupInfoNCE, reaching state-of-the-art performance on a number of biased datasets, including real instances of biases "in the wild".</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE ISBI</abbr> </div> <div id="sarfati_learning_2023" class="col-sm-8"> <div class="title">Learning to diagnose cirrhosis from radiological and histological labels with joint self and weakly-supervised pretraining strategies</div> <div class="author"> Emma Sarfati, Alexandre Bone, Marc-Michel Rohe, <em>Pietro Gori</em>, and Isabelle Bloch </div> <div class="periodical"> <em>In IEEE 20th International Symposium on Biomedical Imaging (ISBI)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1109/ISBI53787.2023.10230783" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://arxiv.org/pdf/2302.08427" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Identifying cirrhosis is key to correctly assess the health of the liver. However, the gold standard diagnosis of the cirrhosis needs a medical intervention to obtain the histological confirmation, e.g. the METAVIR score, as the radiological presentation can be equivocal. In this work, we propose to leverage transfer learning from large datasets annotated by radiologists, which we consider as a weak annotation, to predict the histological score available on a small annex dataset. To this end, we propose to compare different pretraining methods, namely weakly-supervised and self-supervised ones, to improve the prediction of the cirrhosis. Finally, we introduce a loss function combining both supervised and self-supervised frameworks for pretraining. This method outperforms the baseline classification of the METAVIR score, reaching an AUC of 0.84 and a balanced accuracy of 0.75, compared to 0.77 and 0.72 for a baseline classifier.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE ISBI</abbr> </div> <div id="barbano_contrastive_2023" class="col-sm-8"> <div class="title">Contrastive learning for regression in multi-site brain age prediction</div> <div class="author"> Carlo Alberto Barbano, Benoit Dufumier, Edouard Duchesnay, Marco Grangetto, and <em>Pietro Gori</em> </div> <div class="periodical"> <em>In IEEE 20th International Symposium on Biomedical Imaging (ISBI)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2211.08326" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/EIDOSLAB/contrastive-brain-age-prediction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Building accurate Deep Learning (DL) models for brain age prediction is a very relevant topic in neuroimaging, as it could help better understand neurodegenerative disorders and find new biomarkers. To estimate accurate and generalizable models, large datasets have been collected, which are often multi-site and multi-scanner. This large heterogeneity negatively affects the generalization performance of DL models since they are prone to overfit site-related noise. Recently, contrastive learning approaches have been shown to be more robust against noise in data or labels. For this reason, we propose a novel contrastive learning regression loss for robust brain age prediction using MRI scans. Our method achieves state-of-the-art performance on the OpenBHB challenge, yielding the best generalization capability and robustness to site-related noise.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="dufumier_integrating_2023" class="col-sm-8"> <div class="title">Integrating Prior Knowledge in Contrastive Learning with Kernel</div> <div class="author"> Benoit Dufumier, Carlo Alberto Barbano, Robin Louiset, Edouard Duchesnay, and <em>Pietro Gori</em> </div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.mlr.press/v202/dufumier23a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/Duplums/contrastive-decoupled-uniformity" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI-W</abbr> </div> <div id="ruppli_decoupled_2023" class="col-sm-8"> <div class="title">Decoupled conditional contrastive learning with variable metadata for prostate lesion detection</div> <div class="author"> Camille Ruppli, <em>Pietro Gori</em>, Roberto Ardon, and Isabelle Bloch </div> <div class="periodical"> <em>In MILLanD workshop (MICCAI)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://link.springer.com/chapter/10.1007/978-3-031-44917-8_9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/camilleruppli/decoupled_ccl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI-W</abbr> </div> <div id="ruppli_optimizing_2022" class="col-sm-8"> <div class="title">Optimizing Transformations for Contrastive Learning in a Differentiable Framework</div> <div class="author"> Camille Ruppli, <em>Pietro Gori</em>, Roberto Ardon, and Isabelle Bloch </div> <div class="periodical"> <em>In Medical Image Learning with Limited and Noisy Data - MILLanD (Workshop MICCAI)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1007/978-3-031-16760-7_10" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://arxiv.org/pdf/2207.13367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Current contrastive learning methods use random transformations sampled from a large list of transformations, with fixed hyper-parameters, to learn invariance from an unannotated database. Following previous works that introduce a small amount of supervision, we propose a framework to find optimal transformations for contrastive learning using a differentiable transformation network. Our method increases performances at low annotated data regime both in supervision accuracy and in convergence speed. In contrast to previous work, no generative model is needed for transformation optimization. Transformed images keep relevant information to solve the supervised task, here classification. Experiments were performed on 34000 2D slices of brain Magnetic Resonance Images and 11200 chest X-ray images. On both datasets, with 10% of labeled data, our model achieves better performances than a fully supervised model with 100% labels.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> </div> <div id="dufumier_contrastive_2021" class="col-sm-8"> <div class="title">Contrastive Learning with Continuous Proxy Meta-data for 3D MRI Classification</div> <div class="author"> Benoit Dufumier, <em>Pietro Gori</em>, Julie Victor, Antoine Grigis, and Edouard Duchesnay </div> <div class="periodical"> <em>In Medical Image Computing and Computer Assisted Intervention - MICCAI</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-87196-3_6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/Duplums/yAwareContrastiveLearning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Traditional supervised learning with deep neural networks requires a tremendous amount of labelled data to converge to a good solution. For 3D medical images, it is often impractical to build a large homogeneous annotated dataset for a specific pathology. Self-supervised methods offer a new way to learn a representation of the images in an unsupervised manner with a neural network. In particular, contrastive learning has shown great promises by (almost) matching the performance of fully-supervised CNN on vision tasks. Nonetheless, this method does not take advantage of available meta-data, such as participant’s age, viewed as prior knowledge. Here, we propose to leverage continuous proxy metadata, in the contrastive learning framework, by introducing a new loss called y-Aware InfoNCE loss. Specifically, we improve the positive sampling during pre-training by adding more positive examples with similar proxy meta-data with the anchor, assuming they share similar discriminative semantic features. With our method, a 3D CNN model pre-trained on \\10^4\\104multi-site healthy brain MRI scans can extract relevant features for three classification tasks: schizophrenia, bipolar diagnosis and Alzheimer’s detection. When fine-tuned, it also outperforms 3D CNN trained from scratch on these tasks, as well as state-of-the-art self-supervised methods. Our code is made publicly available here.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MedNeurIPS</abbr> </div> <div id="dufumier_conditional_2021" class="col-sm-8"> <div class="title">Conditional Alignment and Uniformity for Contrastive Learning with Continuous Proxy Labels</div> <div class="author"> Benoit Dufumier, <em>Pietro Gori</em>, Julie Victor, Antoine Grigis, and Edouard Duchesnay </div> <div class="periodical"> <em>In MedNeurIPS, Workshop NeurIPS</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://nips.cc/virtual/2021/36825" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Contrastive Learning has shown impressive results on natural and medical images, without requiring annotated data. However, a particularity of medical images is the availability of meta-data (such as age or sex) that can be exploited for learning representations. Here, we show that the recently proposed contrastive y-Aware InfoNCE loss, that integrates multi-dimensional meta-data, asymptotically optimizes two properties: conditional alignment and global uniformity. Similarly to [Wang, 2020], conditional alignment means that similar samples should have similar features, but conditionally on the meta-data. Instead, global uniformity means that the (normalized) features should be uniformly distributed on the unit hyper-sphere, independently of the meta-data. Here, we propose to define conditional uniformity, relying on the meta-data, that repel only samples with dissimilar meta-data. We show that direct optimization of both conditional alignment and uniformity improves the representations, in terms of linear evaluation, on both CIFAR-100 and a brain MRI dataset.</p> </div> </div> </div> </li> </ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Pietro Gori. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: April 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>